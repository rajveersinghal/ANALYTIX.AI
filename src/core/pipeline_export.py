from typing import Any
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, RobustScaler
import joblib
from pathlib import Path
from modules.utils import Logger

def create_sklearn_pipeline(preprocessing_steps: list, model: Any, scaler_type: str = 'robust') -> Pipeline:
    """
    Creates a complete sklearn Pipeline with preprocessing and model.
    
    Args:
        preprocessing_steps: List of (name, transformer) tuples.
        model: Trained sklearn model.
        scaler_type: 'robust' or 'standard'.
        
    Returns:
        sklearn.pipeline.Pipeline object.
    """
    steps = []
    
    # Add scaler
    if scaler_type == 'robust':
        steps.append(('scaler', RobustScaler()))
    else:
        steps.append(('scaler', StandardScaler()))
    
    # Add custom preprocessing steps
    steps.extend(preprocessing_steps)
    
    # Add model
    steps.append(('model', model))
    
    pipeline = Pipeline(steps)
    Logger.log(f"âœ… Created sklearn Pipeline with {len(steps)} steps")
    
    return pipeline

def export_pipeline(pipeline: Pipeline, name: str, metadata: dict = None) -> str:
    """
    Exports sklearn Pipeline to disk.
    
    Args:
        pipeline: sklearn Pipeline object.
        name: Name for the pipeline file.
        metadata: Optional metadata dictionary.
        
    Returns:
        str: Path to saved pipeline.
    """
    save_dir = Path("saved_models")
    save_dir.mkdir(exist_ok=True)
    
    filepath = save_dir / f"{name}_pipeline.joblib"
    joblib.dump(pipeline, filepath)
    
    # Save metadata
    if metadata:
        import json
        meta_path = save_dir / f"{name}_pipeline_metadata.json"
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    Logger.log(f"ðŸ’¾ Pipeline exported: {filepath.name}")
    return str(filepath)

def generate_pipeline_code(pipeline: Pipeline, feature_names: list) -> str:
    """
    Generates Python code to use the exported pipeline.
    
    Args:
        pipeline: sklearn Pipeline.
        feature_names: List of feature names.
        
    Returns:
        str: Python code template.
    """
    code = f'''"""
Production Deployment Code for sklearn Pipeline
Generated by ANALYTIX.AI
"""

import joblib
import pandas as pd
import numpy as np

# Load the pipeline
pipeline = joblib.load("saved_models/pipeline.joblib")

# Example: Make predictions on new data
def predict(data_dict):
    """
    Make predictions using the trained pipeline.
    
    Args:
        data_dict: Dictionary with feature values
        
    Returns:
        Prediction value
    """
    # Convert input to DataFrame
    df = pd.DataFrame([data_dict])
    
    # Ensure correct feature order
    required_features = {feature_names}
    df = df[required_features]
    
    # Make prediction (pipeline handles all preprocessing)
    prediction = pipeline.predict(df)[0]
    
    # Get probability if classification
    if hasattr(pipeline, 'predict_proba'):
        probability = pipeline.predict_proba(df)[0]
        return {{
            'prediction': prediction,
            'probability': probability.tolist()
        }}
    
    return {{'prediction': prediction}}

# Example usage:
if __name__ == "__main__":
    sample_data = {{
        # Add your feature values here
        {", ".join([f"'{f}': 0.0" for f in feature_names[:3]])}
        # ... add remaining features
    }}
    
    result = predict(sample_data)
    print(f"Prediction: {{result}}")
'''
    return code
